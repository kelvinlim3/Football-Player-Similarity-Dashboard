{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "51db6808",
   "metadata": {},
   "outputs": [],
   "source": [
    "# preliminaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.mixture import GaussianMixture\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import skew, kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a36b9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# paths to dataframe csv files\n",
    "DF_FULL_PATH = 'Data/df_full.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bdcd8acb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframes for each position\n",
    "df_full = pd.read_csv(DF_FULL_PATH, index_col=0)\n",
    "cb_df = df_full[df_full.position=='CB']\n",
    "fb_df = df_full[df_full.position=='FB']\n",
    "dm_df = df_full[df_full.position=='DM']\n",
    "m_df = df_full[df_full.position=='M']\n",
    "w_df = df_full[df_full.position=='W']\n",
    "cf_df = df_full[df_full.position=='CF']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "df5fe6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "RAW_TRAITS = ['goals', 'shots', 'conversion', 'positioning', 'assists', 'crossing', 'dribbling', 'carries',\n",
    "              'involvement', 'accuracy', 'intent', 'receiving', 'aerial', 'on_ball', 'off_ball', 'fouls']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "b61a3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get raw traits per position as numpy array\n",
    "cb_raw_traits = cb_df[RAW_TRAITS].to_numpy()\n",
    "fb_raw_traits = fb_df[RAW_TRAITS].to_numpy()\n",
    "dm_raw_traits = dm_df[RAW_TRAITS].to_numpy()\n",
    "m_raw_traits = m_df[RAW_TRAITS].to_numpy()\n",
    "w_raw_traits = w_df[RAW_TRAITS].to_numpy()\n",
    "cf_raw_traits = cf_df[RAW_TRAITS].to_numpy()\n",
    "\n",
    "raw_traits_dict = {'CB': cb_raw_traits,\n",
    "                   'FB': fb_raw_traits,\n",
    "                   'DM': dm_raw_traits,\n",
    "                   'M': m_raw_traits,\n",
    "                   'W': w_raw_traits,\n",
    "                   'CF': cf_raw_traits\n",
    "                  }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "026da7e8",
   "metadata": {},
   "source": [
    "### Gaussian Mixture Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3b188eb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create models\n",
    "gmm_cb = GaussianMixture(n_components=cb_raw_traits.shape[0]//20)\n",
    "gmm_fb = GaussianMixture(n_components=fb_raw_traits.shape[0]//20)\n",
    "gmm_dm = GaussianMixture(n_components=dm_raw_traits.shape[0]//20)\n",
    "gmm_m = GaussianMixture(n_components=m_raw_traits.shape[0]//20)\n",
    "gmm_w = GaussianMixture(n_components=w_raw_traits.shape[0]//20)\n",
    "gmm_cf = GaussianMixture(n_components=cf_raw_traits.shape[0]//20)\n",
    "\n",
    "# fit models\n",
    "gmm_cb = gmm_cb.fit(cb_raw_traits)\n",
    "gmm_fb = gmm_fb.fit(fb_raw_traits)\n",
    "gmm_dm = gmm_dm.fit(dm_raw_traits)\n",
    "gmm_m = gmm_m.fit(m_raw_traits)\n",
    "gmm_w = gmm_w.fit(w_raw_traits)\n",
    "gmm_cf = gmm_cf.fit(cf_raw_traits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "21529c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get clusters for each position\n",
    "gmm_cb_cluster = gmm_cb.predict(cb_raw_traits)\n",
    "gmm_fb_cluster = gmm_fb.predict(fb_raw_traits)\n",
    "gmm_dm_cluster = gmm_dm.predict(dm_raw_traits)\n",
    "gmm_m_cluster = gmm_m.predict(m_raw_traits)\n",
    "gmm_w_cluster = gmm_w.predict(w_raw_traits)\n",
    "gmm_cf_cluster = gmm_cf.predict(cf_raw_traits)\n",
    "\n",
    "# get scores for each position\n",
    "gmm_cb_cluster_probs = gmm_cb.predict_proba(cb_raw_traits)\n",
    "gmm_fb_cluster_probs = gmm_fb.predict_proba(fb_raw_traits)\n",
    "gmm_dm_cluster_probs = gmm_dm.predict_proba(dm_raw_traits)\n",
    "gmm_m_cluster_probs = gmm_m.predict_proba(m_raw_traits)\n",
    "gmm_w_cluster_probs = gmm_w.predict_proba(w_raw_traits)\n",
    "gmm_cf_cluster_probs = gmm_cf.predict_proba(cf_raw_traits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "82af8018",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9999998601739764"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gmm_cb_cluster_probs[0][46]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a924666d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_cb_cluster_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "19cd2ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gmm_scores_cb = np.zeros((cb_raw_traits.shape[0], cb_raw_traits.shape[0]))\n",
    "for i in range(cb_raw_traits.shape[0]):\n",
    "    for j in range(cb_raw_traits.shape[0]):\n",
    "        cluster = gmm_cb_cluster[j]\n",
    "        gmm_cb_scores[i][j] = gmm_cb_cluster_probs[i][cluster]\n",
    "\n",
    "gmm_scores_fb = np.zeros((fb_raw_traits.shape[0], fb_raw_traits.shape[0]))\n",
    "for i in range(fb_raw_traits.shape[0]):\n",
    "    for j in range(fb_raw_traits.shape[0]):\n",
    "        cluster = gmm_fb_cluster[j]\n",
    "        gmm_fb_scores[i][j] = gmm_fb_cluster_probs[i][cluster]\n",
    "\n",
    "gmm_scores_dm = np.zeros((dm_raw_traits.shape[0], dm_raw_traits.shape[0]))\n",
    "for i in range(dm_raw_traits.shape[0]):\n",
    "    for j in range(dm_raw_traits.shape[0]):\n",
    "        cluster = gmm_dm_cluster[j]\n",
    "        gmm_dm_scores[i][j] = gmm_dm_cluster_probs[i][cluster]\n",
    "        \n",
    "gmm_scores_m = np.zeros((m_raw_traits.shape[0], m_raw_traits.shape[0]))\n",
    "for i in range(m_raw_traits.shape[0]):\n",
    "    for j in range(m_raw_traits.shape[0]):\n",
    "        cluster = gmm_m_cluster[j]\n",
    "        gmm_m_scores[i][j] = gmm_m_cluster_probs[i][cluster]\n",
    "        \n",
    "gmm_scores_w = np.zeros((w_raw_traits.shape[0], w_raw_traits.shape[0]))\n",
    "for i in range(w_raw_traits.shape[0]):\n",
    "    for j in range(w_raw_traits.shape[0]):\n",
    "        cluster = gmm_w_cluster[j]\n",
    "        gmm_w_scores[i][j] = gmm_w_cluster_probs[i][cluster]\n",
    "        \n",
    "gmm_scores_cf = np.zeros((cf_raw_traits.shape[0], cf_raw_traits.shape[0]))\n",
    "for i in range(cf_raw_traits.shape[0]):\n",
    "    for j in range(cf_raw_traits.shape[0]):\n",
    "        cluster = gmm_cf_cluster[j]\n",
    "        gmm_cf_scores[i][j] = gmm_cf_cluster_probs[i][cluster]\n",
    "\n",
    "gmm_dict = {'CB': gmm_scores_cb,\n",
    "            'FB': gmm_scores_fb,\n",
    "            'DM': gmm_scores_dm,\n",
    "            'M': gmm_scores_m,\n",
    "            'W': gmm_scores_w,\n",
    "            'CF': gmm_scores_cf\n",
    "           }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc70ae4a",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "4238d26f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_1(scores, raw_traits):\n",
    "    \n",
    "    # get list of predicted ranks of queried players\n",
    "    queried_player_ranks = []\n",
    "    for queried_player_index, scores in enumerate(scores):\n",
    "        player_indices = np.argpartition(scores, -len(raw_traits))\n",
    "        player_indices = np.flip(player_indices[np.argsort(scores[player_indices])])\n",
    "        \n",
    "        for rank, index in enumerate(player_indices):\n",
    "            if index == queried_player_index:\n",
    "                queried_player_ranks.append(rank+1)\n",
    "    \n",
    "    return np.average(queried_player_ranks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "68ab71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_2(scores, raw_traits, top_n=20):\n",
    "\n",
    "    # get list of all averaged ratio stds for each queried player\n",
    "    avg_ratio_stds = []\n",
    "    for queried_player_index, scores in enumerate(scores):\n",
    "\n",
    "        queried_traits = raw_traits[queried_player_index]\n",
    "        queried_traits[np.where(queried_traits==0)] = 0.001 # small epsilon to avoid division by zero\n",
    "        \n",
    "        top_n_indices = np.argpartition(scores, -top_n-1)[-top_n-1:]\n",
    "        top_n_indices = np.flip(top_n_indices[np.argsort(scores[top_n_indices])])\n",
    "        \n",
    "        # get list of ratio std's between queried player and ranked player\n",
    "        ratio_stds = []\n",
    "        for ind in top_n_indices:\n",
    "            # stop loop when top_n players have been processed\n",
    "            if len(ratio_stds) == top_n:\n",
    "                break\n",
    "            # skip queried player\n",
    "            if ind == queried_player_index:\n",
    "                continue\n",
    "                \n",
    "            player_traits = raw_traits[ind]\n",
    "            ratios = player_traits / queried_traits\n",
    "            ratio_std = ratios.std()\n",
    "            ratio_stds.append(ratio_std)\n",
    "\n",
    "        # compute average of ratio stds\n",
    "        avg_ratio_stds.append(np.average(ratio_stds))\n",
    "\n",
    "    return np.average(avg_ratio_stds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b839c677",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_3(scores):\n",
    "    # compute variances of all queries\n",
    "    all_variances = np.var(scores, axis=1)\n",
    "    \n",
    "    return np.average(all_variances)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0dc7f8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_4(scores):\n",
    "    # compute skewness of all queries\n",
    "    all_skews = skew(scores, axis=1)\n",
    "    \n",
    "    return np.average(all_skews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "94bfd476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_5(scores):\n",
    "    # compute kurtosis of all queries\n",
    "    all_kurtosis = kurtosis(scores, axis=1)\n",
    "    \n",
    "    return np.average(all_kurtosis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "d267c87c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_6(scores):\n",
    "    # get min and max of all scores\n",
    "    min_score = scores.min()\n",
    "    max_score = scores.max()\n",
    "    \n",
    "    return [min_score, max_score]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b71a5fcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "methods_dict = {'Gaussian Mixture Model': gmm_dict}\n",
    "dynamicity_dict = {'Gaussian Mixture Model': 'Y'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "c76f6b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {'Method': [],\n",
    "           'Average rank of queried player': [],\n",
    "           'Standard deviation of trait ratios of top 20 similar players': [],\n",
    "           'Scores sparsity': [],\n",
    "           'Skewness of scores': [],\n",
    "           'Kurtosis of scores': [],\n",
    "           'Range of scores': [],\n",
    "           'Dynamicity': []\n",
    "          }\n",
    "\n",
    "for name, dictionary in methods_dict.items():\n",
    "    results['Method'].append(name)\n",
    "    results['Dynamicity'].append(dynamicity_dict[name])\n",
    "    eval_1_list = []\n",
    "    eval_2_list = []\n",
    "    eval_3_list = []\n",
    "    eval_4_list = []\n",
    "    eval_5_list = []\n",
    "    min_score = 99999\n",
    "    max_score = -99999\n",
    "    for pos, scores in dictionary.items():\n",
    "        raw_traits = raw_traits_dict[pos]\n",
    "        eval_1_list.append(eval_1(scores, raw_traits))\n",
    "        eval_2_list.append(eval_2(scores, raw_traits))\n",
    "        eval_3_list.append(eval_3(scores))\n",
    "        eval_4_list.append(eval_4(scores))\n",
    "        eval_5_list.append(eval_5(scores))\n",
    "        temp_min_score, temp_max_score = eval_6(scores)\n",
    "        min_score = min(min_score, temp_min_score)\n",
    "        max_score = max(max_score, temp_max_score)\n",
    "    results['Average rank of queried player'].append(np.average(eval_1_list))\n",
    "    results['Standard deviation of trait ratios of top 20 similar players'].append(np.average(eval_2_list))\n",
    "    results['Scores sparsity'].append(np.average(eval_3_list))\n",
    "    results['Skewness of scores'].append(np.average(eval_4_list))\n",
    "    results['Kurtosis of scores'].append(np.average(eval_5_list))\n",
    "    results['Range of scores'].append([round(min_score, 2), round(max_score, 2)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e1438018",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c0d3e164",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Method</th>\n",
       "      <th>Average rank of queried player</th>\n",
       "      <th>Standard deviation of trait ratios of top 20 similar players</th>\n",
       "      <th>Scores sparsity</th>\n",
       "      <th>Skewness of scores</th>\n",
       "      <th>Kurtosis of scores</th>\n",
       "      <th>Range of scores</th>\n",
       "      <th>Dynamicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Gaussian Mixture Model</td>\n",
       "      <td>13.291785</td>\n",
       "      <td>0.462423</td>\n",
       "      <td>0.024403</td>\n",
       "      <td>6.762616</td>\n",
       "      <td>48.512136</td>\n",
       "      <td>[0.0, 1.0]</td>\n",
       "      <td>Y</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                   Method  Average rank of queried player  \\\n",
       "0  Gaussian Mixture Model                       13.291785   \n",
       "\n",
       "   Standard deviation of trait ratios of top 20 similar players  \\\n",
       "0                                           0.462423              \n",
       "\n",
       "   Scores sparsity  Skewness of scores  Kurtosis of scores Range of scores  \\\n",
       "0         0.024403            6.762616           48.512136      [0.0, 1.0]   \n",
       "\n",
       "  Dynamicity  \n",
       "0          Y  "
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ee3ad4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
